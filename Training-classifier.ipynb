{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d29285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss, BCELoss\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score\n",
    "import pickle\n",
    "from transformers import *\n",
    "from tqdm import tqdm, trange\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff73a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e18a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv').dropna() #jigsaw-toxic-comment-classification-challenge\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c8e50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unique comments: ', df.news.nunique() == df.shape[0])\n",
    "print('Null values: ', df.isnull().values.any())\n",
    "# df[df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9512c102",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns\n",
    "label_cols = list(cols[1:])\n",
    "num_labels = len(label_cols)\n",
    "print('Label columns: ', label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f1da89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True) #shuffle rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77f7710",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['one_hot_labels'] = list(df[label_cols].values)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1291e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(df.one_hot_labels.values)\n",
    "comments = list(df.news.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be528c4f",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d3a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) # tokenizer\n",
    "encodings = tokenizer.batch_encode_plus(comments,max_length=max_length,pad_to_max_length=True) # tokenizer's encoding method\n",
    "print('tokenizer outputs: ', encodings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a6b051",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encodings['input_ids'] # tokenized and encoded sentences\n",
    "token_type_ids = encodings['token_type_ids'] # token type ids\n",
    "attention_masks = encodings['attention_mask'] # attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54848b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying indices of 'one_hot_labels' entries that only occur once - this will allow us to stratify split our training data later\n",
    "label_counts = df.one_hot_labels.astype(str).value_counts()\n",
    "one_freq = label_counts[label_counts==1].keys()\n",
    "one_freq_idxs = sorted(list(df[df.one_hot_labels.astype(str).isin(one_freq)].index), reverse=True)\n",
    "print('df label indices with only one instance: ', one_freq_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a87241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering single instance inputs to force into the training set after stratified split\n",
    "one_freq_input_ids = [input_ids.pop(i) for i in one_freq_idxs]\n",
    "one_freq_token_types = [token_type_ids.pop(i) for i in one_freq_idxs]\n",
    "one_freq_attention_masks = [attention_masks.pop(i) for i in one_freq_idxs]\n",
    "one_freq_labels = [labels.pop(i) for i in one_freq_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34e57a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels, train_token_types, validation_token_types, train_masks, validation_masks = train_test_split(input_ids, labels, token_type_ids,attention_masks,\n",
    "                                                            random_state=2020, test_size=0.10, stratify = labels)\n",
    "\n",
    "# Add one frequency data to train data\n",
    "train_inputs.extend(one_freq_input_ids)\n",
    "train_labels.extend(one_freq_labels)\n",
    "train_masks.extend(one_freq_attention_masks)\n",
    "train_token_types.extend(one_freq_token_types)\n",
    "\n",
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "train_token_types = torch.tensor(train_token_types)\n",
    "\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "validation_token_types = torch.tensor(validation_token_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a02e150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning with XLNet, the authors recommend a batch size of 32, 48, or 128. We will use 32 here to avoid memory issues.\n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels, train_token_types)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels, validation_token_types)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0c4f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(validation_dataloader,'validation_data_loader')\n",
    "torch.save(train_dataloader,'train_data_loader')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6773179c",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d24239",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_labels)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29738f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting custom optimization parameters. You may implement a scheduler here as well.\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab319ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters,lr=2e-5,correct_bias=True)\n",
    "# optimizer = AdamW(model.parameters(),lr=2e-5)  # Default optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d169c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 10\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "\n",
    "  # Training\n",
    "  \n",
    "  # Set our model to training mode (as opposed to evaluation mode)\n",
    "  model.train()\n",
    "\n",
    "  # Tracking variables\n",
    "  tr_loss = 0 #running loss\n",
    "  nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "  # Train the data for one epoch\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels, b_token_types = batch\n",
    "    # Clear out the gradients (by default they accumulate)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # # Forward pass for multiclass classification\n",
    "    # outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "    # loss = outputs[0]\n",
    "    # logits = outputs[1]\n",
    "\n",
    "    # Forward pass for multilabel classification\n",
    "    #outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "    logits = outputs[0]\n",
    "    loss_func = BCEWithLogitsLoss() \n",
    "    loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "    # loss_func = BCELoss() \n",
    "    # loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "    train_loss_set.append(loss.item())    \n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    # Update parameters and take a step using the computed gradient\n",
    "    optimizer.step()\n",
    "    # scheduler.step()\n",
    "    # Update tracking variables\n",
    "    tr_loss += loss.item()\n",
    "    nb_tr_examples += b_input_ids.size(0)\n",
    "    nb_tr_steps += 1\n",
    "\n",
    "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "  # Validation\n",
    "\n",
    "  # Put model in evaluation mode to evaluate loss on the validation set\n",
    "  model.eval()\n",
    "\n",
    "  # Variables to gather full output\n",
    "  logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n",
    "\n",
    "  # Predict\n",
    "  for i, batch in enumerate(validation_dataloader):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels, b_token_types = batch\n",
    "    with torch.no_grad():\n",
    "      # Forward pass\n",
    "      #outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "      outs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "      b_logit_pred = outs[0]\n",
    "      pred_label = torch.sigmoid(b_logit_pred)\n",
    "\n",
    "      b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "      pred_label = pred_label.to('cpu').numpy()\n",
    "      b_labels = b_labels.to('cpu').numpy()\n",
    "\n",
    "    tokenized_texts.append(b_input_ids)\n",
    "    logit_preds.append(b_logit_pred)\n",
    "    true_labels.append(b_labels)\n",
    "    pred_labels.append(pred_label)\n",
    "\n",
    "  # Flatten outputs\n",
    "  pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "  true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "  # Calculate Accuracy\n",
    "  threshold = 0.50\n",
    "  pred_bools = [pl>threshold for pl in pred_labels]\n",
    "  true_bools = [tl==1 for tl in true_labels]\n",
    "  val_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')*100\n",
    "  val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n",
    "\n",
    "  print('F1 Validation Accuracy: ', val_f1_accuracy)\n",
    "  print('Flat Validation Accuracy: ', val_flat_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1a7d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./distilbert_model_news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac53362",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44195a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mod = DistilBertForSequenceClassification.from_pretrained(\"./distilbert_model_news\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad40f04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_statement = \"India failed to avoid a series whitewash against a South African team in transition despite Deepak Chahar's scintillating 34-ball 54, losing the third ODI by four runs here on Sunday to end a disastrous tour of the Rainbow Nation.Asked to take first strike, South Africa scored 287, thanks to Quinton de Kock's aggressive hundred and Rassie van der Dussen’s fluent half-century. In repl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b524ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = tokenizer(test_statement,max_length=max_length,pad_to_max_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6377ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids = tok['input_ids']\n",
    "\n",
    "test_token_type_ids = tok['token_type_ids']\n",
    "test_attention_masks = tok['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86019a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = torch.tensor(test_input_ids).to(device).resize_(1,100)\n",
    "#test_labels = torch.tensor(test_labels)\n",
    "test_masks = torch.tensor(test_attention_masks).to(device).resize_(1,100)\n",
    "test_token_types = torch.tensor(test_token_type_ids).to(device).resize_(1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb3d52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c55d365",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c629b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outs = model.forward(test_inputs, token_type_ids=None, attention_mask=test_masks)\n",
    "outs = model.forward(test_inputs, attention_mask=test_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda6fc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sigmoid(outs[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
